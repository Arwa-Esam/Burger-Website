{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDx1_r460mtB"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install captum"
      ],
      "metadata": {
        "id": "mGoLch8D8dk0",
        "outputId": "5553d894-5cad-4e1c-9b19-f47d00d5c849",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting captum\n",
            "  Downloading captum-0.6.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from captum) (1.24.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from captum) (3.2.2)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.8/dist-packages (from captum) (2.0.0.dev20230129+cu117)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.6->captum) (3.0)\n",
            "Requirement already satisfied: pytorch-triton==2.0.0+0d7e753227 in /usr/local/lib/python3.8/dist-packages (from torch>=1.6->captum) (2.0.0+0d7e753227)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.6->captum) (4.4.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=1.6->captum) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from pytorch-triton==2.0.0+0d7e753227->torch>=1.6->captum) (3.9.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.8/dist-packages (from pytorch-triton==2.0.0+0d7e753227->torch>=1.6->captum) (3.25.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->captum) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->captum) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->captum) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->captum) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->captum) (1.15.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=1.6->captum) (1.2.1)\n",
            "Installing collected packages: captum\n",
            "Successfully installed captum-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "import torch"
      ],
      "metadata": {
        "id": "0MbDmPiZ3rdf",
        "outputId": "bfd617e4-337a-4c71-8d46-1efd1929ad3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.1+cu116)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (0.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.25.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "NE6XW0IV50YT",
        "outputId": "e32515bd-fd12-4823-d020-15c33f600f6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.1+cu116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install numpy --pre torch[dynamo] --force-reinstall --extra-index-url https://download.pytorch.org/whl/nightly/cu117"
      ],
      "metadata": {
        "id": "0US7TQBr56fd",
        "outputId": "ef651ca1-3d66-47ae-9e2e-e491bcca0b90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/nightly/cu117\n",
            "Collecting numpy\n",
            "  Downloading https://download.pytorch.org/whl/nightly/numpy-1.24.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch[dynamo]\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu117/torch-2.0.0.dev20230129%2Bcu117-cp38-cp38-linux_x86_64.whl (1826.6 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 1826562048 bytes == 0x3d26000 @  0x7fd6b6fa8680 0x7fd6b6fc9824 0x5b3128 0x5bbc90 0x5f714c 0x64d800 0x527022 0x504866 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x5f5ee6 0x56bbe1 0x569d8a 0x5f60c3 0x56cc92 0x569d8a 0x5f60c3\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 2283208704 bytes == 0x70b18000 @  0x7fd6b6fa8680 0x7fd6b6fc8da2 0x5f714c 0x64d800 0x527022 0x504866 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x5f5ee6 0x56bbe1 0x569d8a 0x5f60c3 0x56cc92 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a\n",
            "tcmalloc: large alloc 1826562048 bytes == 0x3d26000 @  0x7fd6b6fa8680 0x7fd6b6fc9824 0x5f97c1 0x5f8ecc 0x504866 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x50b32c 0x5f6b7b 0x66731d 0x5f6706 0x571143 0x50b22e 0x570b82 0x569d8a 0x50b3a0 0x570b82 0x569d8a 0x50b3a0 0x56cc92 0x501044 0x56be83 0x501044 0x56be83 0x501044 0x56be83 0x5f5ee6\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m989.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions\n",
            "  Downloading https://download.pytorch.org/whl/nightly/typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
            "Collecting networkx\n",
            "  Downloading networkx-3.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-triton==2.0.0+0d7e753227\n",
            "  Downloading https://download.pytorch.org/whl/nightly/pytorch_triton-2.0.0%2B0d7e753227-cp38-cp38-linux_x86_64.whl (18.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.7/18.7 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sympy\n",
            "  Downloading https://download.pytorch.org/whl/nightly/sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 KB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading https://download.pytorch.org/whl/nightly/filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
            "Collecting cmake\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cmake-3.25.0-py2.py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting mpmath>=0.19\n",
            "  Downloading https://download.pytorch.org/whl/nightly/mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.6/532.6 KB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mpmath, cmake, typing-extensions, sympy, numpy, networkx, MarkupSafe, filelock, jinja2, torch, pytorch-triton\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.2.1\n",
            "    Uninstalling mpmath-1.2.1:\n",
            "      Successfully uninstalled mpmath-1.2.1\n",
            "  Attempting uninstall: cmake\n",
            "    Found existing installation: cmake 3.22.6\n",
            "    Uninstalling cmake-3.22.6:\n",
            "      Successfully uninstalled cmake-3.22.6\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.4.0\n",
            "    Uninstalling typing_extensions-4.4.0:\n",
            "      Successfully uninstalled typing_extensions-4.4.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.7.1\n",
            "    Uninstalling sympy-1.7.1:\n",
            "      Successfully uninstalled sympy-1.7.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.0\n",
            "    Uninstalling networkx-3.0:\n",
            "      Successfully uninstalled networkx-3.0\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.0.1\n",
            "    Uninstalling MarkupSafe-2.0.1:\n",
            "      Successfully uninstalled MarkupSafe-2.0.1\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.9.0\n",
            "    Uninstalling filelock-3.9.0:\n",
            "      Successfully uninstalled filelock-3.9.0\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 2.11.3\n",
            "    Uninstalling Jinja2-2.11.3:\n",
            "      Successfully uninstalled Jinja2-2.11.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 2.0.0.dev20230129+cu117 which is incompatible.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 2.0.0.dev20230129+cu117 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 2.0.0.dev20230129+cu117 which is incompatible.\n",
            "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.24.1 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.1 which is incompatible.\n",
            "notebook 5.7.16 requires jinja2<=3.0.0, but you have jinja2 3.1.2 which is incompatible.\n",
            "flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.1.2 which is incompatible.\n",
            "fastai 2.7.10 requires torch<1.14,>=1.7, but you have torch 2.0.0.dev20230129+cu117 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-2.1.2 cmake-3.25.0 filelock-3.9.0 jinja2-3.1.2 mpmath-1.2.1 networkx-3.0 numpy-1.24.1 pytorch-triton-2.0.0+0d7e753227 sympy-1.11.1 torch-2.0.0.dev20230129+cu117 typing-extensions-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import captum\n",
        "from captum.attr import IntegratedGradients, Occlusion, LayerGradCam, LayerAttribution\n",
        "from captum.attr import visualization as viz\n",
        "\n",
        "import os, sys\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap"
      ],
      "metadata": {
        "id": "V3BWFkmt381v"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "class ToyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(3, 3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.lin2 = nn.Linear(3, 2)\n",
        "\n",
        "        # initialize weights and biases\n",
        "        self.lin1.weight = nn.Parameter(torch.arange(-4.0, 5.0).view(3, 3))\n",
        "        self.lin1.bias = nn.Parameter(torch.zeros(1,3))\n",
        "        self.lin2.weight = nn.Parameter(torch.arange(-3.0, 3.0).view(2, 3))\n",
        "        self.lin2.bias = nn.Parameter(torch.ones(1,2))\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.lin2(self.relu(self.lin1(input)))\n",
        "\n",
        "\n",
        "model = ToyModel()\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "YrKHXHib4Q5-",
        "outputId": "22fef116-93ed-4c49-9896-2d91a86c65ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ToyModel(\n",
              "  (lin1): Linear(in_features=3, out_features=3, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (lin2): Linear(in_features=3, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "np.random.seed(123)"
      ],
      "metadata": {
        "id": "y2OJk4yd84wY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.rand(2, 3)\n",
        "baseline = torch.zeros(2, 3)"
      ],
      "metadata": {
        "id": "9iOeBb5j865E"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ig = IntegratedGradients(model)\n",
        "attributions, delta = ig.attribute(input, baseline, target=0, return_convergence_delta=True)\n",
        "print('IG Attributions:', attributions)\n",
        "print('Convergence Delta:', delta)"
      ],
      "metadata": {
        "id": "9Z2OcOeH888r",
        "outputId": "4adf38f7-1214-408a-b604-d879282c325f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IG Attributions: tensor([[-0.5922, -1.5497, -1.0067],\n",
            "        [ 0.0000, -0.2219, -5.1991]], dtype=torch.float64)\n",
            "Convergence Delta: tensor([ 5.9605e-08, -8.8818e-16], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1OvoAiRA89Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV1JcJUu0mtN"
      },
      "source": [
        "\n",
        "# Model Interpretability using Captum\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmrraPXN0mtS"
      },
      "source": [
        "Captum helps you understand how the data features impact your model\n",
        "predictions or neuron activations, shedding light on how your model\n",
        "operates.\n",
        "\n",
        "Using Captum, you can apply a wide range of state-of-the-art feature\n",
        "attribution algorithms such as \\ ``Guided GradCam``\\  and\n",
        "\\ ``Integrated Gradients``\\  in a unified way.\n",
        "\n",
        "In this recipe you will learn how to use Captum to: \n",
        "\n",
        "- Attribute the predictions of an image classifier to their corresponding image features. \n",
        "- Visualize the attribution results.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J5jhDC00mtT"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9KCV3GS0mtU"
      },
      "source": [
        "Make sure Captum is installed in your active Python environment. Captum\n",
        "is available both on GitHub, as a ``pip`` package, or as a ``conda``\n",
        "package. For detailed instructions, consult the installation guide at\n",
        "https://captum.ai/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVbITX8c0mtU"
      },
      "source": [
        "For a model, we use a built-in image classifier in PyTorch. Captum can\n",
        "reveal which parts of a sample image support certain predictions made by\n",
        "the model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TD3GxJ-V0mtV"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "model = torchvision.models.resnet18(pretrained=True).eval()\n",
        "\n",
        "response = requests.get(\"https://image.freepik.com/free-photo/two-beautiful-puppies-cat-dog_58409-6024.jpg\")\n",
        "img = Image.open(BytesIO(response.content))\n",
        "\n",
        "center_crop = transforms.Compose([\n",
        " transforms.Resize(256),\n",
        " transforms.CenterCrop(224),\n",
        "])\n",
        "\n",
        "normalize = transforms.Compose([\n",
        "    transforms.ToTensor(),               # converts the image to a tensor with values between 0 and 1\n",
        "    transforms.Normalize(                # normalize to follow 0-centered imagenet pixel rgb distribution\n",
        "     mean=[0.485, 0.456, 0.406],\n",
        "     std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "input_img = normalize(center_crop(img)).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gbR28b10mtY"
      },
      "source": [
        "## Computing Attribution\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDxxl3qh0mtd"
      },
      "source": [
        "Among the top-3 predictions of the models are classes 208 and 283 which\n",
        "correspond to dog and cat.\n",
        "\n",
        "Let us attribute each of these predictions to the corresponding part of\n",
        "the input, using Captum’s \\ ``Occlusion``\\  algorithm.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lW_CYQz0mtf"
      },
      "outputs": [],
      "source": [
        "from captum.attr import Occlusion \n",
        "\n",
        "occlusion = Occlusion(model)\n",
        "\n",
        "strides = (3, 9, 9)               # smaller = more fine-grained attribution but slower\n",
        "target=208,                       # Labrador index in ImageNet \n",
        "sliding_window_shapes=(3,45, 45)  # choose size enough to change object appearance\n",
        "baselines = 0                     # values to occlude the image with. 0 corresponds to gray\n",
        "\n",
        "attribution_dog = occlusion.attribute(input_img,\n",
        "                                       strides = strides,\n",
        "                                       target=target,\n",
        "                                       sliding_window_shapes=sliding_window_shapes,\n",
        "                                       baselines=baselines)\n",
        "\n",
        "\n",
        "target=283,                       # Persian cat index in ImageNet \n",
        "attribution_cat = occlusion.attribute(input_img,\n",
        "                                       strides = strides,\n",
        "                                       target=target,\n",
        "                                       sliding_window_shapes=sliding_window_shapes,\n",
        "                                       baselines=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siFasuKJ0mth"
      },
      "source": [
        "Besides ``Occlusion``, Captum features many algorithms such as\n",
        "\\ ``Integrated Gradients``\\ , \\ ``Deconvolution``\\ ,\n",
        "\\ ``GuidedBackprop``\\ , \\ ``Guided GradCam``\\ , \\ ``DeepLift``\\ , and\n",
        "\\ ``GradientShap``\\ . All of these algorithms are subclasses of\n",
        "``Attribution`` which expects your model as a callable ``forward_func``\n",
        "upon initialization and has an ``attribute(...)`` method which returns\n",
        "the attribution result in a unified format.\n",
        "\n",
        "Let us visualize the computed attribution results in case of images.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR5lEjyg0mti"
      },
      "source": [
        "## Visualizing the Results\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc3wIM6q0mtj"
      },
      "source": [
        "Captum’s \\ ``visualization``\\  utility provides out-of-the-box methods\n",
        "to visualize attribution results both for pictorial and for textual\n",
        "inputs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e52G_CuM0mtl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from captum.attr import visualization as viz\n",
        "\n",
        "# Convert the compute attribution tensor into an image-like numpy array\n",
        "attribution_dog = np.transpose(attribution_dog.squeeze().cpu().detach().numpy(), (1,2,0))\n",
        "\n",
        "vis_types = [\"heat_map\", \"original_image\"]\n",
        "vis_signs = [\"all\", \"all\"] # \"positive\", \"negative\", or \"all\" to show both\n",
        "# positive attribution indicates that the presence of the area increases the prediction score\n",
        "# negative attribution indicates distractor areas whose absence increases the score\n",
        "\n",
        "_ = viz.visualize_image_attr_multiple(attribution_dog,\n",
        "                                      np.array(center_crop(img)),\n",
        "                                      vis_types,\n",
        "                                      vis_signs,\n",
        "                                      [\"attribution for dog\", \"image\"],\n",
        "                                      show_colorbar = True\n",
        "                                     )\n",
        "\n",
        "\n",
        "attribution_cat = np.transpose(attribution_cat.squeeze().cpu().detach().numpy(), (1,2,0))\n",
        "\n",
        "_ = viz.visualize_image_attr_multiple(attribution_cat,\n",
        "                                      np.array(center_crop(img)),\n",
        "                                      [\"heat_map\", \"original_image\"],  \n",
        "                                      [\"all\", \"all\"], # positive/negative attribution or all\n",
        "                                      [\"attribution for cat\", \"image\"],\n",
        "                                      show_colorbar = True\n",
        "                                     )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNc1rLGO0mtm"
      },
      "source": [
        "If your data is textual, ``visualization.visualize_text()`` offers a\n",
        "dedicated view to explore attribution on top of the input text. Find out\n",
        "more at http://captum.ai/tutorials/IMDB_TorchText_Interpret\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez0d6Mqq0mtn"
      },
      "source": [
        "## Final Notes\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBtyqcX00mtp"
      },
      "source": [
        "Captum can handle most model types in PyTorch across modalities\n",
        "including vision, text, and more. With Captum you can: \\* Attribute a\n",
        "specific output to the model input as illustrated above. \\* Attribute a\n",
        "specific output to a hidden-layer neuron (see Captum API reference). \\*\n",
        "Attribute a hidden-layer neuron response to the model input (see Captum\n",
        "API reference).\n",
        "\n",
        "For complete API of the supported methods and a list of tutorials,\n",
        "consult our website http://captum.ai\n",
        "\n",
        "Another useful post by Gilbert Tanner:\n",
        "https://gilberttanner.com/blog/interpreting-pytorch-models-with-captum\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qCfvW6LV2sig"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}